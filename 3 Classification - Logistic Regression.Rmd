---
title: "Classification"
author: "Marco Rizk"
date: "7 December 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Script for different classification methods

```{r}
#loading packages
library(ISLR)
library(GGally)
```

```{r}
#loading dataset
names(Smarket)
ggcorr(Smarket)
```
```{r}
plot(Smarket$Volume)
```

##Logistic Regression

```{r}
#using generalized linear model function
m1 = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,
         family = binomial) #family=binomial means logisitc regression
summary(m1)
```
least Pr value is for Lag1 corresponding to a -ve coeffiecent which indicates that if the market had a positive return yesterday, its likely to go ip today. but Pr is large to suggest any association
```{r}
#accessing model coefficents (2 methods)
coef(m1)
summary(m1)$coef
```
```{r}
#predicting probabilities
glm.probs = predict(m1,type = "response")
glm.probs[1:10]
```

```{r}
#creating a dummy variable for contrasts
contrasts(Smarket$Direction)
```
```{r}
#converting predicted probabilites to labels using dummy variable
glm.pred=rep("Down",1250)#creating a 1250 length vector with value "Down"
glm.pred[glm.probs>0.5]="Up" #converting P>0.5 to up

#creating a confustion matrix
table(glm.pred,Smarket$Direction)
```
```{r}
#splitting data into training and test sets
train = (Smarket$Year<2005)
Smarket.2005 = Smarket[!train,]#boolean index vector
Direction.2005=Smarket$Direction[!train]
```

```{r}
#refiting the model to training set
m1 = glm(Direction???Lag1+Lag2+Lag3+Lag4+Lag5+Volume ,
data=Smarket ,family=binomial ,subset=train)

glm.probs=predict(m1,Smarket.2005,type="response")
```


```{r}
#assessing prediction on test set
glm.pred = rep("Down",252)
glm.pred[glm.probs>0.5]="Up"
table(glm.pred,Direction.2005)
#correct prediction percentage
mean(glm.pred == Direction.2005)

```
```{r}
#Tring to improve the model by removing the high p-value parameters
glm.fit=glm(Direction~Lag1+Lag2 ,data=Smarket ,family=binomial ,
subset=train)
glm.probs=predict (glm.fit ,Smarket.2005, type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]=" Up"
table(glm.pred ,Direction.2005)
mean(glm.pred == Direction.2005)
```
```{r}
#making predictions for scpecific input values
predict (glm.fit ,newdata =data.frame(Lag1=c(1.2 ,1.5),
Lag2=c(1.1,-0.8) ),type="response")
```

##Linear Discreminant Analysis
```{r}
library(MASS)
lda.fit = lda(Direction~Lag1+Lag2 ,data=Smarket ,subset=train)
lda.fit
```

```{r}
#making predictions
lda.pred = predict(lda.fit,Smarket.2005)
table(lda.pred$class,Direction.2005)
mean(lda.pred$class==Direction.2005)


```

## Quadratic Discriminant Analysis

```{r}
qda.fit=qda(Direction???Lag1+Lag2 ,data=Smarket ,subset=train)
qda.fit
```

```{r}
qda.class=predict (qda.fit ,Smarket.2005) $class
table(qda.class ,Direction.2005)
mean(qda.class ==Direction.2005)

```

## K Nearest Neighbour
Because the KNN classifier predicts the class of a given test observation by
identifying the observations that are nearest to it, the scale of the variables
matters. Any variables that are on a large scale will have a much larger
effect on the distance between the observations, and hence on the KNN
classifier, than variables that are on a small scale.
A good way to handle this problem is to standardize the data so that all standardize
variables are given a mean of zero and a standard deviation of one. Then
all variables will be on a comparable scale. The scale() function does just scale() this
```{r}
library(class)
#standarazing values
standardized.X= scale(Caravan [,-86])
var(Caravan [ ,1])

var(Caravan [ ,2])

var(standardized.X[,1])

var(standardized.X[,2])

```

```{r}
#input matrices
test=1:1000
train.X= standardized.X[-test ,]
test.X= standardized.X[test ,]
train.Y=Caravan$Purchase [-test]
test.Y=Caravan$Purchase [test]
```

```{r}
set.seed(1)
knn.pred=knn(train.X,test.X,train.Y,k=5)
mean(test.Y!=knn.pred)
table(knn.pred ,test.Y)
```

