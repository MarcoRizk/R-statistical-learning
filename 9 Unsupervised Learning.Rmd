---
title: "Unsupervised Learning"
author: "Marco Rizk"
date: "18 December 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Unsupervised Learning

In this lab, we perform PCA on the USArrests data set, which is part of
the base R package. The rows of the data set contain the 50 states, in
alphabetical order.

```{r}
states=row.names(USArrests )
states
```


The columns of the data set contain the four variables.


```{r}
names(USArrests )

```

We first briefly examine the data. We notice that the variables have vastly
different means.

```{r}
apply(USArrests , 2, mean)
```
Note that the apply() function allows us to apply a function-in this case,
the mean() function-to each row or column of the data set. The second
input here denotes whether we wish to compute the mean of the rows, 1,
or the columns, 2. We see that there are on average three times as many
rapes as murders, and more than eight times as many assaults as rapes.
We can also examine the variances of the four variables using the apply()
function.

```{r}
apply(USArrests , 2, var)
```

Not surprisingly, the variables also have vastly different variances: the
UrbanPop variable measures the percentage of the population in each state
living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. If we failed to scale the
variables before performing PCA, then most of the principal components
that we observed would be driven by the Assault variable, since it has by
far the largest mean and variance. Thus, it is important to standardize the
variables to have mean zero and standard deviation one before performing
PCA.
We now perform principal components analysis using the prcomp() func- prcomp() tion, which is one of several functions in R that perform PCA

```{r}
pr.out=prcomp(USArrests , scale=TRUE)
```

By default, the prcomp() function centers the variables to have mean zero.
By using the option scale=TRUE, we scale the variables to have standard
deviation one. The output from prcomp() contains a number of useful quantities.
```{r}
names(pr.out)
```


The center and scale components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA.

```{r}
pr.out$center

pr.out$scale
```

The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component
loading vector

```{r}
pr.out$rotation
```
We see that there are four distinct principal components. This is to be
expected because there are in general min(n ??? 1, p) informative principal
components in a data set with n observations and p variables.

Using the prcomp() function, we do not need to explicitly multiply the
data by the principal component loading vectors in order to obtain the
principal component score vectors. Rather the 50 × 4 matrix x has as its
columns the principal component score vectors. That is, the kth column is
the kth principal component score vector.

```{r}
dim(pr.out$x)
```

We can plot the first two principal components as follows:


```{r}
 biplot (pr.out , scale =0)
```

The scale=0 argument to biplot() ensures that the arrows are scaled to biplot() represent the loadings; other values for scale give slightly different biplots
with different interpretations.
Notice that this figure is a mirror image of Figure 10.1. Recall that
the principal components are only unique up to a sign change, so we can
reproduce Figure 10.1 by making a few small changes:

```{r}
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot (pr.out , scale =0)
```
The prcomp() function also outputs the standard deviation of each principal component. For instance, on the USArrests data set, we can access
these standard deviations as follows:

```{r}
 pr.out$sdev

```




The variance explained by each principal component is obtained by squaring these:
```{r}
pr.var=pr.out$sdev ^2
pr.var
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component
by the total variance explained by all four principal components

```{r}
pve=pr.var/sum(pr.var)
pve
```
We see that the first principal component explains 62.0 % of the variance
in the data, the next principal component explains 24.7 % of the variance,
and so forth. We can plot the PVE explained by each component, as well
as the cumulative PVE, as follows:

```{r}
plot(pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,1),type="b")
plot(cumsum(pve), xlab="Principal Component ", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1),
type="b")
```
The result is shown in Figure 10.4. Note that the function cumsum() comcumsum() putes the cumulative sum of the elements of a numeric vector. For instance

```{r}
a=c(1,2,8,-3)
cumsum(a)
```

## Clustering

### K-means Clustering

The function kmeans() performs K-means clustering in R. We begin with kmeans() a simple simulated example in which there truly are two clusters in the
data: the first 25 observations have a mean shift relative to the next 25
observations.

```{r}
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
```
We now perform K-means clustering with K = 2.


```{r}
 km.out=kmeans (x,2, nstart =20)
```
The cluster assignments of the 50 observations are contained in
km.out$cluster

```{r}
km.out$cluster
```
The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kmeans(). We
can plot the data, with each observation colored according to its cluster
assignment.

```{r}
plot(x, col=(km.out$cluster +1), main="K-Means Clustering
Results with K=2", xlab="", ylab="", pch=20, cex=2)
```
Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.
In this example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with K = 3.

```{r}
set.seed(4)
km.out=kmeans (x,3, nstart =20)
km.out
```



When K = 3, K-means clustering splits up the two clusters.
To run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument. If a value of nstart greater than one
is used, then K-means clustering will be performed using multiple random
assignments in Step 1 of Algorithm 10.1, and the kmeans() function will
report only the best results. Here we compare using nstart=1 to nstart=20

```{r}
 set.seed(3)
km.out=kmeans (x,3, nstart =1)
km.out$tot.withinss

km.out=kmeans (x,3, nstart =20)
km.out$tot.withinss
```

Note that km.out$tot.withinss is the total within-cluster sum of squares,
which we seek to minimize by performing K-means clustering (Equation
10.11). The individual within-cluster sum-of-squares are contained in the
vector km.out$withinss.
We strongly recommend always running K-means clustering with a large
value of nstart, such as 20 or 50, since otherwise an undesirable local
optimum may be obtained.
When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the
set.seed() function. This way, the initial cluster assignments in Step 1 can
be replicated, and the K-means output will be fully reproducible.

### Hierarchical Clustering

The hclust() function implements hierarchical clustering in R. In the fol- hclust() lowing example we use the data from Section 10.5.1 to plot the hierarchical
clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by
clustering observations using complete linkage. The dist() function is used
dist() to compute the 50 × 50 inter-observation Euclidean distance matrix.

```{r}
hc.complete =hclust(dist(x), method="complete")
```

We could just as easily perform hierarchical clustering with average or
single linkage instead:

```{r}
hc.average =hclust(dist(x), method ="average")
hc.single=hclust(dist(x), method ="single")
```

We can now plot the dendrograms obtained using the usual plot() function.
The numbers at the bottom of the plot identify each observation.

```{r}
par(mfrow=c(1,3))
plot(hc.complete ,main="Complete Linkage ", xlab="", sub="",
cex=.9)
plot(hc.average , main="Average Linkage", xlab="", sub="",
cex=.9)
plot(hc.single , main="Single Linkage ", xlab="", sub="",
cex=.9)
```

To determine the cluster labels for each observation associated with a
given cut of the dendrogram, we can use the cutree() function:

To scale the variables before performing hierarchical clustering of the
observations, we use the scale() function:

```{r}
xsc=scale(x)
plot(hclust(dist(xsc), method ="complete"), main=" Hierarchical
Clustering with Scaled Features ")
```
Correlation-based distance can be computed using the as.dist() funcas.dist() tion, which converts an arbitrary square symmetric matrix into a form that
the hclust() function recognizes as a distance matrix. However, this only
makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is
always 1. Hence, we will cluster a three-dimensional data set.


```{r}
x=matrix(rnorm (30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method ="complete"), main=" Complete Linkage
with Correlation -Based Distance ", xlab="", sub ="")

```

