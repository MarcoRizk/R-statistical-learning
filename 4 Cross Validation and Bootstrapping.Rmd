---
title: "Cross-Validation and the Bootstrap"
author: "Marco Rizk"
date: "10 December 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Cross-Validation and the Bootstrap

```{r}
library(ISLR)
set.seed(1)
train=sample (392,196)
```
```{r}
#creating a linear regression model
lm.fit=lm(mpg???horsepower ,data=Auto ,subset=train)
```


```{r}
attach(Auto)
#calculating the MSE for validation set
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
```

```{r}
#Assessing MSE for polynomial models
lm.fit2=lm(mpg???poly(horsepower ,2),data=Auto , subset=train)
mean((mpg -predict (lm.fit2 ,Auto ))[- train]^2)

lm.fit3=lm(mpg???poly(horsepower ,3),data=Auto , subset=train)
mean((mpg -predict (lm.fit3 ,Auto ))[- train]^2)
```

```{r}
#testing MSE using different test/validation sets
set.seed(2)
train=sample (392,196)

lm.fit=lm(mpg???horsepower ,subset=train)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)

lm.fit2=lm(mpg???poly(horsepower ,2),data=Auto , subset=train)
mean((mpg -predict (lm.fit2 ,Auto ))[- train]^2)

lm.fit3=lm(mpg???poly(horsepower ,3),data=Auto , subset=train)
mean((mpg -predict (lm.fit3 ,Auto ))[- train]^2)
```

##Leave One out validation

```{r}
#using glm() instead of lm() grants access to its validation methods
library(boot)
glm.fit=glm(mpg~horsepower ,data=Auto)
cv.err=cv.glm(Auto ,glm.fit)
#get LOOV results
cv.err$delta
```

```{r}
#using cross validation in a for loop to test the least error in different polynomial models
cv.error=rep(0,5)
for (i in 1:5){
 glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
 cv.error[i]=cv.glm(Auto ,glm.fit)$delta [1]
 }
cv.error
```
it shows that the 2nd degree model showed substantial decrease that receeded after

## k-Fold Cross-Validation

```{r}
set.seed(17)
#testing with k =10
cv.error.10=rep(0 ,10)
for (i in 1:10){
 glm.fit=glm(mpg???poly(horsepower ,i),data=Auto)
 cv.error.10[i]=cv.glm(Auto ,glm.fit ,K=10)$delta[1]
 }
cv.error.10
```


#  The Bootstrap

```{r}
#create a function that computes the statistic of interest ??
alpha.fn=function (data ,index){
 X=data$X[index]
 Y=data$Y[index]
 return ((var(Y)-cov(X,Y))/(var(X)+var(Y) -2*cov(X,Y)))
 }
```

```{r}
#using the function to estimate alpha for 100 samples
alpha.fn(Portfolio ,1:100)
```
```{r}
#using bootstrap to take 1000 samples to estimate alpha
boot(Portfolio ,alpha.fn,R=1000)
```

### Estimating the Accuracy of a Linear Regression Model using bootstrap

```{r}
#boot function to return model coefficients of a model
boot.fn=function (data ,index)
 return(coef(lm(mpg~horsepower ,data=data , subset=index)))
```

```{r}
#applying bootstrap on the function
boot(Auto ,boot.fn ,1000)
```

this estimation of standard error is more accurate than the one given by the summary() function